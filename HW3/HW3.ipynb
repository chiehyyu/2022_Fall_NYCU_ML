{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3: Decision Tree, AdaBoost and Random Forest\n",
    "In hw3, you need to implement decision tree, adaboost and random forest by using only numpy, then train your implemented model by the provided dataset. TA will use the on-hold test label to evaluate your model performance.\n",
    "\n",
    "Please note that only **NUMPY** can be used to implement your model, you will get no points by simply calling `sklearn.tree.DecisionTreeClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Gini Index or Entropy is often used for measuring the “best” splitting of the data. Please compute the Entropy and Gini Index of provided data. Please use the formula from [page 5 of hw3 slides](https://docs.google.com/presentation/d/1kIe_-YZdemRMmr_3xDy-l0OS2EcLgDH7Uan14tlU5KE/edit#slide=id.gd542a5ff75_0_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the gini index from the given labels\n",
    "def gini(sequence):\n",
    "    # count the appearance of label 1\n",
    "    cnt = 0\n",
    "    for i in range(len(sequence)):\n",
    "        if sequence[i] == 1: cnt += 1\n",
    "    p1 = cnt / len(sequence) # probability of label 1\n",
    "    p2 = 1 - p1 # probability of the other label\n",
    "    n = 1 - p1 ** 2 - p2 ** 2\n",
    "    return n\n",
    "\n",
    "# compute the entropy from the given labels\n",
    "def entropy(sequence):\n",
    "    # count the appearance of label 1\n",
    "    cnt = 0\n",
    "    for i in range(len(sequence)):\n",
    "        if sequence[i] == 1: cnt += 1\n",
    "    p1 = cnt / len(sequence) # probability of label 1\n",
    "    p2 = 1 - p1 # probability of the other label\n",
    "    # handle the case of getting a list having the same label\n",
    "    # to prevent processing zero values in p1 & p2\n",
    "    if not p1 or not p2: return 0\n",
    "    else:\n",
    "        n = - (p1 * np.log2(p1)) - (p2 * np.log2(p2))\n",
    "        return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 = class 1, 2 = class 2\n",
    "data = np.array([1,2,1,1,1,1,2,2,1,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini of data is  0.4628099173553719\n"
     ]
    }
   ],
   "source": [
    "print(\"Gini of data is \", gini(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of data is  0.9456603046006401\n"
     ]
    }
   ],
   "source": [
    "print(\"Entropy of data is \", entropy(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "It is a binary classifiation dataset that classify if price is high or not for a cell phone, the label is stored in `price_range` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 21)\n",
      "(300, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>battery_power</th>\n",
       "      <th>blue</th>\n",
       "      <th>clock_speed</th>\n",
       "      <th>dual_sim</th>\n",
       "      <th>fc</th>\n",
       "      <th>four_g</th>\n",
       "      <th>int_memory</th>\n",
       "      <th>m_dep</th>\n",
       "      <th>mobile_wt</th>\n",
       "      <th>n_cores</th>\n",
       "      <th>...</th>\n",
       "      <th>px_height</th>\n",
       "      <th>px_width</th>\n",
       "      <th>ram</th>\n",
       "      <th>sc_h</th>\n",
       "      <th>sc_w</th>\n",
       "      <th>talk_time</th>\n",
       "      <th>three_g</th>\n",
       "      <th>touch_screen</th>\n",
       "      <th>wifi</th>\n",
       "      <th>price_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1883</td>\n",
       "      <td>1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>95</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>531</td>\n",
       "      <td>791</td>\n",
       "      <td>3724</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>987</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>0.5</td>\n",
       "      <td>83</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>419</td>\n",
       "      <td>736</td>\n",
       "      <td>2757</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1306</td>\n",
       "      <td>1</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>0.4</td>\n",
       "      <td>174</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>867</td>\n",
       "      <td>1258</td>\n",
       "      <td>2521</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1611</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>98</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>303</td>\n",
       "      <td>714</td>\n",
       "      <td>1595</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>862</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>0.4</td>\n",
       "      <td>165</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>793</td>\n",
       "      <td>1758</td>\n",
       "      <td>278</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   battery_power  blue  clock_speed  dual_sim  fc  four_g  int_memory  m_dep  \\\n",
       "0           1883     1          2.7         1   6       1          30    0.5   \n",
       "1            987     1          1.9         0   4       1          52    0.5   \n",
       "2           1306     1          2.1         1   2       1          33    0.4   \n",
       "3           1611     1          0.5         1  11       0           3    0.6   \n",
       "4            862     1          0.8         1   3       0          23    0.4   \n",
       "\n",
       "   mobile_wt  n_cores  ...  px_height  px_width   ram  sc_h  sc_w  talk_time  \\\n",
       "0         95        2  ...        531       791  3724    16    15         20   \n",
       "1         83        3  ...        419       736  2757    17    12         15   \n",
       "2        174        3  ...        867      1258  2521     6     5         16   \n",
       "3         98        3  ...        303       714  1595     8     4          9   \n",
       "4        165        2  ...        793      1758   278    16     3         11   \n",
       "\n",
       "   three_g  touch_screen  wifi  price_range  \n",
       "0        1             1     1            1  \n",
       "1        1             1     0            1  \n",
       "2        1             0     0            1  \n",
       "3        1             0     1            0  \n",
       "4        0             0     1            0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "val_df = pd.read_csv('val.csv')\n",
    "\n",
    "x_train = train_df.drop('price_range', axis=1)\n",
    "y_train = train_df.loc[:, \"price_range\"]\n",
    "x_val = val_df.drop('price_range', axis=1)\n",
    "y_val = val_df.loc[:, \"price_range\"]\n",
    "\n",
    "print(train_df.shape)\n",
    "print(val_df.shape)\n",
    "\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Implement the Decision Tree algorithm (CART, Classification and Regression Trees) and train the model by the given arguments, and print the accuracy score on the validation data. You should implement two arguments for the Decision Tree algorithm\n",
    "1. **criterion**: The function to measure the quality of a split. Your model should support `gini` for the Gini impurity and `entropy` for the information gain. \n",
    "2. **max_depth**: The maximum depth of the tree. If `max_depth=None`, then nodes are expanded until all leaves are pure. `max_depth=1` equals to split data once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    def __init__(self, criterion='gini', max_depth=None):\n",
    "        # store the criterion and max_depth for future usage\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    # split the dataset based on an attribute's value\n",
    "    # return the left and right child\n",
    "    def try_split(self, index, value, dataset):\n",
    "        left, right = list(), list()\n",
    "        for row in dataset:\n",
    "            if row[index] <= value: left.append(row)\n",
    "            else: right.append(row)\n",
    "        return left, right\n",
    "\n",
    "    # random sample attributes' index for random forest\n",
    "    def sample_features(self, dataset, max_features):\n",
    "        sample_index = np.random.choice(len(dataset[0]) - 1, max_features, replace=False)\n",
    "        return sample_index\n",
    "\n",
    "    # select the best split\n",
    "    def select_split(self, dataset):\n",
    "        # select the best split from the random sampled features for random forest\n",
    "        if self.random_forest == True:\n",
    "            # setup initial values for a node:\n",
    "            #   index: the attribute's index\n",
    "            #   value: the threshold used to split\n",
    "            #   score: purity after spliting (weighted average gini-index or entropy)\n",
    "            #   left & right: left & right child\n",
    "            tmp_index, tmp_value, tmp_score, tmp_left, tmp_right = 999, 9999, 999, None, None\n",
    "            sample_index = self.sample_features(dataset, max_features=self.max_features)\n",
    "            for i in sample_index: # iterate from random sampled index\n",
    "                thresholds = set([row[i] for row in dataset]) # pick out unique values for the certain attribute\n",
    "                for t in thresholds:\n",
    "                    if t == max(thresholds): continue\n",
    "                    left, right = self.try_split(i, t, dataset)\n",
    "                    left_labels = [row[-1] for row in left]\n",
    "                    right_labels = [row[-1] for row in right]\n",
    "                    if self.criterion == 'gini':\n",
    "                        score = (gini(left_labels) * len(left) + gini(right_labels) * len(right)) / (len(left) + len(right))\n",
    "                    else: score = (entropy(left_labels) * len(left) + entropy(right_labels) * len(right)) / (len(left) + len(right))\n",
    "                    # find the split that yields the lowest value of gini or entropy\n",
    "                    if score < tmp_score: # score stores the minimum score in the end\n",
    "                        tmp_index, tmp_value, tmp_score, tmp_left, tmp_right = i, t, score, left, right\n",
    "            return {'index':tmp_index, 'value':tmp_value, 'left':tmp_left, 'right':tmp_right}\n",
    "        else:\n",
    "            tmp_index, tmp_value, tmp_score, tmp_left, tmp_right = 999, 9999, 999, None, None\n",
    "            for i in range(len(dataset[0]) - 1): # iterate from all index\n",
    "                thresholds = set([row[i] for row in dataset]) # pick out unique values for the certain attribute\n",
    "                for t in thresholds:\n",
    "                    if t == max(thresholds): continue\n",
    "                    left, right = self.try_split(i, t, dataset)\n",
    "                    left_labels = [row[-1] for row in left]\n",
    "                    right_labels = [row[-1] for row in right]\n",
    "                    if self.criterion == 'gini':\n",
    "                        score = (gini(left_labels) * len(left) + gini(right_labels) * len(right)) / (len(left) + len(right))\n",
    "                    else: score = (entropy(left_labels) * len(left) + entropy(right_labels) * len(right)) / (len(left) + len(right))\n",
    "                    # find the split that yields the lowest value of gini or entropy\n",
    "                    if score < tmp_score: # score stores the minimum score in the end\n",
    "                        tmp_index, tmp_value, tmp_score, tmp_left, tmp_right = i, t, score, left, right\n",
    "            return {'index':tmp_index, 'value':tmp_value, 'left':tmp_left, 'right':tmp_right}\n",
    "\n",
    "    # create a leaf node\n",
    "    def leaf(self, list):\n",
    "        labels = [row[-1] for row in list]\n",
    "        return max(labels, key=labels.count)\n",
    "\n",
    "    # recursively split the tree and create leaf nodes\n",
    "    def split(self, node, depth):\n",
    "        left, right = node['left'], node['right']\n",
    "\n",
    "        # one side is empty after split\n",
    "        if not left or not right:\n",
    "            node['left'] = node['right'] = self.leaf(left + right)\n",
    "            return\n",
    "        \n",
    "        # max depth constraint\n",
    "        if (self.max_depth != None):\n",
    "            if depth >= self.max_depth:\n",
    "                node['left'], node['right'] = self.leaf(left), self.leaf(right)\n",
    "                return\n",
    "        \n",
    "        # handle left\n",
    "        left_labels = [row[-1] for row in left]\n",
    "        # all belongs to the same class\n",
    "        if not left_labels.count(0) or not left_labels.count(1):\n",
    "            node['left'] = self.leaf(left)\n",
    "        else:\n",
    "            node['left'] = self.select_split(left)\n",
    "            # if cannot split anymore (index got the initial value)\n",
    "            if node['left']['index'] == 999:\n",
    "                node['left'] = self.leaf(left)\n",
    "            # proceed to split\n",
    "            else: self.split(node['left'], depth + 1)\n",
    "        \n",
    "        # handle right\n",
    "        right_labels = [row[-1] for row in right]\n",
    "        # all belongs to the same class\n",
    "        if not right_labels.count(0) or not right_labels.count(1):\n",
    "            node['right'] = self.leaf(right)\n",
    "        else:\n",
    "            node['right'] = self.select_split(right)\n",
    "            # if cannot split anymore (index got the initial value)\n",
    "            if node['right']['index'] == 999:\n",
    "                node['right'] = self.leaf(right)\n",
    "            # proceed to split\n",
    "            else: self.split(node['right'], depth + 1)       \n",
    "\n",
    "    # build the decision tree\n",
    "    def fit(self, x_data, y_data, random_forest=False, max_features=0):\n",
    "        # determine whether we are building random forest\n",
    "        self.random_forest = random_forest\n",
    "        self.max_features = int(max_features)\n",
    "        if isinstance(x_data, pd.DataFrame): \n",
    "            x_data = x_data.values.tolist()\n",
    "            y_data = y_data.values.tolist()\n",
    "        # merge x_data and y_data to dataset\n",
    "        dataset = [list(x_data[i]) + [y_data[i]] for i in range(len(x_data))]\n",
    "        # start to split the tree\n",
    "        self.root = self.select_split(dataset)\n",
    "        self.split(self.root, 1)\n",
    "\n",
    "    # make prediction for one row / instance\n",
    "    def predict_instance(self, node, row):\n",
    "        if row[node['index']] <= node['value']:\n",
    "            if isinstance(node['left'], dict):\n",
    "                return self.predict_instance(node['left'], row)\n",
    "            else: return node['left']\n",
    "        else:\n",
    "            if isinstance(node['right'], dict):\n",
    "                return self.predict_instance(node['right'], row)\n",
    "            else: return node['right']    \n",
    "\n",
    "    # return the array of prediction labels\n",
    "    def predict(self, x_data):\n",
    "        y_predict = []\n",
    "        if isinstance(x_data, pd.DataFrame): x_data = x_data.values.tolist()\n",
    "        for row in x_data:\n",
    "            y_predict.append(self.predict_instance(self.root, row))\n",
    "        y_predict = np.array(y_predict)\n",
    "        return y_predict\n",
    "\n",
    "    # count how many times each feature are chosen to split\n",
    "    def count_feature(self, list, node, x_data):\n",
    "        if isinstance(node, dict):\n",
    "            list[node['index']] += 1\n",
    "            self.count_feature(list, node['left'], x_data)\n",
    "            self.count_feature(list, node['right'], x_data)\n",
    "        return list\n",
    "\n",
    "    # print tree for debugging use\n",
    "    # def print_tree(self, node, depth=0):\n",
    "    #     if isinstance(node, dict):\n",
    "    #         print('%s[Attribute %d < %.3f]' % ((depth*'\\t', (node['index']), node['value'])))\n",
    "    #         self.print_tree(node['left'], depth + 1)\n",
    "    #         self.print_tree(node['right'], depth + 1)\n",
    "    #     else:\n",
    "    #         print('%s[%s]' % ((depth*'\\t', node)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1\n",
    "Using `criterion=gini`, showing the accuracy score of validation data by `max_depth=3` and `max_depth=10`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth=3 accuracy:  0.9166666666666666\n",
      "max_depth=10 accuracy:  0.9366666666666666\n"
     ]
    }
   ],
   "source": [
    "# build the decision tree\n",
    "clf_depth3 = DecisionTree(criterion='gini', max_depth=3)\n",
    "clf_depth3.fit(x_train, y_train)\n",
    "# calculate the accuracy score\n",
    "print(\"max_depth=3 accuracy: \", accuracy_score(y_val, clf_depth3.predict(x_val)))\n",
    "\n",
    "# build the decision tree\n",
    "clf_depth10 = DecisionTree(criterion='gini', max_depth=10)\n",
    "clf_depth10.fit(x_train, y_train)\n",
    "# calculate the accuracy score\n",
    "print(\"max_depth=10 accuracy: \", accuracy_score(y_val, clf_depth10.predict(x_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2\n",
    "Using `max_depth=3`, showing the accuracy score of validation data by `criterion=gini` and `criterion=entropy`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "criterion=gini accuracy:  0.9166666666666666\n",
      "criterion=entropy accuracy:  0.93\n"
     ]
    }
   ],
   "source": [
    "# build the decision tree\n",
    "clf_gini = DecisionTree(criterion='gini', max_depth=3)\n",
    "clf_gini.fit(x_train, y_train)\n",
    "# calculate the accuracy score\n",
    "print(\"criterion=gini accuracy: \", accuracy_score(y_val, clf_gini.predict(x_val)))\n",
    "\n",
    "# build the decision tree\n",
    "clf_entropy = DecisionTree(criterion='entropy', max_depth=3)\n",
    "clf_entropy.fit(x_train, y_train)\n",
    "# calculate the accuracy score\n",
    "print(\"criterion=entropy accuracy: \", accuracy_score(y_val, clf_entropy.predict(x_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: Your decisition tree scores should over **0.9**. It may suffer from overfitting, if so, you can tune the hyperparameter such as `max_depth`\n",
    "- Note: You should get the same results when re-building the model with the same arguments,  no need to prune the trees\n",
    "- Hint: You can use the recursive method to build the nodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Plot the [feature importance](https://sefiks.com/2020/04/06/feature-importance-in-decision-trees/) of your Decision Tree model. You can get the feature importance by counting the feature used for splitting data.\n",
    "\n",
    "- You can simply plot the **counts of feature used** for building tree without normalize the importance. Take the figure below as example, outlook feature has been used for splitting for almost 50 times. Therefore, it has the largest importance\n",
    "\n",
    "![image](https://i2.wp.com/sefiks.com/wp-content/uploads/2020/04/c45-fi-results.jpg?w=481&ssl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "attributes = [0] * len(x_train.values.tolist()[0])\n",
    "features = clf_depth10.count_feature(attributes, clf_depth10.root, x_train.values.tolist())\n",
    "names = list(x_train.columns)\n",
    "\n",
    "fig, x = plt.subplots(figsize =(10, 10))\n",
    "x.barh(names, features)\n",
    "for s in ['top', 'bottom', 'left', 'right']: x.spines[s].set_visible(False)\n",
    "font = {'size':20}\n",
    "x.set_title(\"Feature Importance\", fontdict=font)\n",
    "x.grid(axis = 'x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "Implement the AdaBooest algorithm by using the CART you just implemented from question 2 as base learner. You should implement one arguments for the AdaBoost.\n",
    "1. **n_estimators**: The maximum number of estimators at which boosting is terminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost():\n",
    "    # initialize the Adaboost \n",
    "    def __init__(self, n_estimators, max_depth=1):\n",
    "        # store n_estimators for future usage\n",
    "        self.n_estimators = n_estimators\n",
    "        # initialize decision trees (the specified amount)\n",
    "        self.trees = [DecisionTree(max_depth)] * n_estimators\n",
    "        # create an array of all alpha value\n",
    "        self.alpha = []\n",
    "\n",
    "    # compute error for the certain weak classifier\n",
    "    def compute_classifier_error(self, tree, x_data, y_data):\n",
    "        sample_index = np.random.choice(len(x_data), len(x_data), replace=True, p=self.w)\n",
    "        sample_index = sample_index.tolist()\n",
    "        boostrap_x = [x_data[i] for i in sample_index]\n",
    "        boostrap_y = [y_data[i] for i in sample_index]\n",
    "        tree.fit(boostrap_x, boostrap_y)\n",
    "        y_pred = tree.predict(x_data)\n",
    "        error = 0\n",
    "        for i in range(len(x_data)):\n",
    "            if y_data[i] != y_pred[i]: error += self.w[i]\n",
    "        return error\n",
    "\n",
    "    # compute weight for the weak classifier\n",
    "    def compute_alpha(self, error):\n",
    "        return np.log((1 - error) / error) / 2\n",
    "\n",
    "    # update data weight according to previous weights and alpha\n",
    "    def update_weight(self, alpha, y_data, y_pred):\n",
    "        flag = []\n",
    "        y_pred = list(y_pred)\n",
    "        for i in range(len(y_data)):\n",
    "            if y_data[i] != y_pred[i]: flag.append(-1)\n",
    "            else: flag.append(1)\n",
    "        flag = np.array(flag)\n",
    "        return self.w * np.exp(-alpha * flag) / sum(self.w)\n",
    "\n",
    "    # train the Adaboost model\n",
    "    def fit(self, x_data, y_data):\n",
    "        if isinstance(x_data, pd.DataFrame):\n",
    "            x_data = x_data.values.tolist()\n",
    "            y_data = y_data.values.tolist()\n",
    "        # initialize uniform weight to each instance\n",
    "        self.w = np.ones(len(x_data)) * 1 / len(x_data)\n",
    "        for i in range(self.n_estimators):\n",
    "            # compute classifier error\n",
    "            clf_error = self.compute_classifier_error(self.trees[i], x_data, y_data)\n",
    "            # weight classifier -> compute alpha\n",
    "            self.alpha.append(self.compute_alpha(clf_error))\n",
    "            # update weight distribution\n",
    "            y_pred = self.trees[i].predict(x_data)\n",
    "            self.update_weight(self.alpha[i], y_data, y_pred)\n",
    "\n",
    "    # make predictions according to the weak classifiers and their weight (alpha)\n",
    "    def predict(self, x_data):\n",
    "        if isinstance(x_data, pd.DataFrame): x_data = x_data.values.tolist()\n",
    "        y_pred = []\n",
    "        for i in range(len(x_data)):\n",
    "            t_pred = 0\n",
    "            for j in range(len(self.trees)):\n",
    "                prediction = self.trees[j].predict_instance(self.trees[j].root, x_data[i])\n",
    "                if prediction == 0: t_pred -= self.alpha[j]\n",
    "                else: t_pred += self.alpha[j]\n",
    "            if t_pred < 0: y_pred.append(0)\n",
    "            else: y_pred.append(1)\n",
    "        y_pred = np.array(y_pred)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1\n",
    "Show the accuracy score of validation data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=10 accuracy:  0.92\n",
      "n_estimators=100 accuracy:  0.94\n"
     ]
    }
   ],
   "source": [
    "# train the Adaboost model\n",
    "ada10 = AdaBoost(10)\n",
    "ada10.fit(x_train, y_train)\n",
    "# calculate the accuracy score\n",
    "print(\"n_estimators=10 accuracy: \", accuracy_score(y_val.values.tolist(), ada10.predict(x_val)))\n",
    "\n",
    "# train the Adaboost model\n",
    "ada100 = AdaBoost(100)\n",
    "ada100.fit(x_train, y_train)\n",
    "# calculate the accuracy score\n",
    "print(\"n_estimators=100 accuracy: \", accuracy_score(y_val.values.tolist(), ada100.predict(x_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "implement the Random Forest algorithm by using the CART you just implemented from question 2. You should implement three arguments for the Random Forest.\n",
    "\n",
    "1. **n_estimators**: The number of trees in the forest. \n",
    "2. **max_features**: The number of random select features to consider when looking for the best split\n",
    "3. **bootstrap**: Whether bootstrap samples are used when building tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    def __init__(self, n_estimators, max_features, boostrap=True, criterion='gini', max_depth=None):\n",
    "        # store variables for future usage\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = max_features\n",
    "        self.boostrap = boostrap\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        # initialize the decision trees (the specified amount)\n",
    "        self.trees = [DecisionTree(max_depth=max_depth)] * n_estimators\n",
    "\n",
    "    # boostrap sampling \n",
    "    def boostrap_sample(self, x_data, y_data):\n",
    "        sample_index = np.random.choice(len(x_data), len(x_data), replace=True)\n",
    "        sample_index = sample_index.tolist()\n",
    "        boostrap_x = [x_data[x] for x in sample_index]\n",
    "        boostrap_y = [y_data[x] for x in sample_index]\n",
    "        return boostrap_x, boostrap_y\n",
    "\n",
    "    # train the random forest model\n",
    "    def fit(self, x_data, y_data):\n",
    "        if isinstance(x_data, pd.DataFrame):\n",
    "            x_data = x_data.values.tolist()\n",
    "            y_data = y_data.values.tolist()\n",
    "        # build different weak classifiers with random sampled data and features\n",
    "        for i in range(self.n_estimators):\n",
    "            if self.boostrap == True:\n",
    "                # random sample data\n",
    "                x, y = self.boostrap_sample(x_data, y_data)\n",
    "            else: x, y = x_data, y_data\n",
    "            self.trees[i].fit(x, y, random_forest=True, max_features=self.max_features)\n",
    "\n",
    "    # make predictions according to the votes from each weak classifier\n",
    "    def predict(self, x_data):\n",
    "        if isinstance(x_data, pd.DataFrame): x_data = x_data.values.tolist()\n",
    "        y_pred = []\n",
    "        for i in range(len(x_data)):\n",
    "            t_pred = [] # the list to store votes\n",
    "            for j in range(len(self.trees)):\n",
    "                prediction = self.trees[j].predict_instance(self.trees[j].root, x_data[i])\n",
    "                t_pred.append(prediction)\n",
    "            # pick out the majority to be the final prediction\n",
    "            majority = max(t_pred, key=t_pred.count)\n",
    "            y_pred.append(majority)\n",
    "        y_pred = np.array(y_pred)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.1\n",
    "Using `criterion=gini`, `max_depth=None`, `max_features=sqrt(n_features)`, showing the accuracy score of validation data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the random forest model\n",
    "clf_10tree = RandomForest(n_estimators=10, max_features=np.sqrt(x_train.shape[1]))\n",
    "clf_100tree = RandomForest(n_estimators=100, max_features=np.sqrt(x_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf_10tree accuracy:  0.8433333333333334\n",
      "clf_100tree accuracy:  0.8466666666666667\n"
     ]
    }
   ],
   "source": [
    "# train the model and calculate accuracy score\n",
    "clf_10tree.fit(x_train, y_train)\n",
    "print(\"clf_10tree accuracy: \", accuracy_score(y_val.values.tolist(), clf_10tree.predict(x_val)))\n",
    "# train the model and calculate accuracy score\n",
    "clf_100tree.fit(x_train, y_train)\n",
    "print(\"clf_100tree accuracy: \", accuracy_score(y_val.values.tolist(), clf_100tree.predict(x_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.2\n",
    "Using `criterion=gini`, `max_depth=None`, `n_estimators=10`, showing the accuracy score of validation data by `max_features=sqrt(n_features)` and `max_features=n_features`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the random forest model\n",
    "clf_random_features = RandomForest(n_estimators=10, max_features=np.sqrt(x_train.shape[1]))\n",
    "clf_all_features = RandomForest(n_estimators=10, max_features=x_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: Use majority votes to get the final prediction, you may get slightly different results when re-building the random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf_random_features accuracy:  0.84\n",
      "clf_all_features accuracy:  0.94\n"
     ]
    }
   ],
   "source": [
    "# train the model and calculate accuracy score\n",
    "clf_random_features.fit(x_train, y_train)\n",
    "print(\"clf_random_features accuracy: \", accuracy_score(y_val.values.tolist(), clf_random_features.predict(x_val)))\n",
    "# train the model and calculate accuracy score\n",
    "clf_all_features.fit(x_train, y_train)\n",
    "print(\"clf_all_features accuracy: \", accuracy_score(y_val.values.tolist(), clf_all_features.predict(x_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6. Train and tune your model on a real-world dataset\n",
    "Try your best to get higher accuracy score of your model. After parameter tuning, you can train your model on the full dataset (train + val).\n",
    "- Feature engineering\n",
    "- Hyperparameter tuning\n",
    "- Implement any other ensemble methods, such as gradient boosting. Please note that you **can not** call any package. Also, only ensemble method can be used. Neural network method is not allowed to used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_your_model(data):\n",
    "    ## Define your model and training\n",
    "    if isinstance(data, pd.DataFrame): data = data.values.tolist()\n",
    "    x_data = [row[0:-1] for row in data]\n",
    "    y_data = [row[-1] for row in data]\n",
    "    my_model = AdaBoost(100)\n",
    "    my_model.fit(x_train, y_train)\n",
    "    return my_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.concat([train_df, val_df])\n",
    "my_model = train_your_model(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = pd.read_csv('x_test.csv')\n",
    "y_pred = my_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('model.pickle', 'wb') as pkl_file:\n",
    "    pickle.dump(my_model, pkl_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "np.save(\"y_pred.npy\", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert y_pred.shape == (500, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary\n",
    "If you have trouble to implement this homework, TA strongly recommend watching [this video](https://www.youtube.com/watch?v=LDRbO9a6XPU), which explains Decision Tree model clearly. But don't copy code from any resources, try to finish this homework by yourself! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DO NOT MODIFY CODE BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'y_test.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/v9/rjjg9sd921jgcyhvrgcn1nt00000gn/T/ipykernel_13188/2371694481.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y_test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price_range'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test-set accuarcy score: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'y_test.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_test = pd.read_csv('y_test.csv')['price_range'].values\n",
    "\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** We will check your result for Question 3 manually *** (5 points)\n",
      "*** We will check your result for Question 6 manually *** (20 points)\n",
      "Approximate score range: 45.0 ~ 70.0\n",
      "*** This score is only for reference ***\n"
     ]
    }
   ],
   "source": [
    "def discrete_checker(score, thres, clf, name, x_train, y_train, x_test, y_test):\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    if accuracy_score(y_test, y_pred) - thres >= 0:\n",
    "        return score\n",
    "    else:\n",
    "        print(f\"{name} failed\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "def patient_checker(score, thres, CLS, kwargs, name,\n",
    "                    x_train, y_train, x_test, y_test, patient=10):\n",
    "    while patient > 0:\n",
    "        patient -= 1\n",
    "        clf = CLS(**kwargs)\n",
    "        clf.fit(x_train, y_train)\n",
    "        y_pred = clf.predict(x_test)\n",
    "        if accuracy_score(y_test, y_pred) - thres >= 0:\n",
    "            return score\n",
    "    print(f\"{name} failed\")\n",
    "    print(\"Considering the randomness, we will check it manually\")\n",
    "    return 0\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    file_url = \"http://storage.googleapis.com/download.tensorflow.org/data/abalone_train.csv\"\n",
    "    df = pd.read_csv(\n",
    "        file_url,\n",
    "        names=[\"Length\", \"Diameter\", \"Height\", \"Whole weight\", \"Shucked weight\",\n",
    "               \"Viscera weight\", \"Shell weight\", \"Age\"]\n",
    "    )\n",
    "\n",
    "    df['Target'] = (df[\"Age\"] > 15).astype(int)\n",
    "    df = df.drop(labels=[\"Age\"], axis=\"columns\")\n",
    "\n",
    "    train_idx = range(0, len(df), 10)\n",
    "    test_idx = range(1, len(df), 20)\n",
    "\n",
    "    train_df = df.iloc[train_idx]\n",
    "    test_df = df.iloc[test_idx]\n",
    "\n",
    "    x_train = train_df.drop(labels=[\"Target\"], axis=\"columns\")\n",
    "    feature_names = x_train.columns.values\n",
    "    x_train = x_train.values\n",
    "    y_train = train_df['Target'].values\n",
    "\n",
    "    x_test = test_df.drop(labels=[\"Target\"], axis=\"columns\")\n",
    "    x_test = x_test.values\n",
    "    y_test = test_df['Target'].values\n",
    "    return x_train, y_train, x_test, y_test, feature_names\n",
    "\n",
    "\n",
    "score = 0\n",
    "\n",
    "data = np.array([1, 2])\n",
    "if abs(gini(data) - 0.5) < 1e-4:\n",
    "    score += 2.5\n",
    "else:\n",
    "    print(\"gini test failed\")\n",
    "\n",
    "if abs(entropy(data) - 1) < 1e-4:\n",
    "    score += 2.5\n",
    "else:\n",
    "    print(\"entropy test failed\")\n",
    "\n",
    "x_train, y_train, x_test, y_test, feature_names = load_dataset()\n",
    "\n",
    "score += discrete_checker(5, 0.9337,\n",
    "                          DecisionTree(criterion='gini', max_depth=3),\n",
    "                          \"DecisionTree(criterion='gini', max_depth=3)\",\n",
    "                          x_train, y_train, x_test, y_test\n",
    "                          )\n",
    "\n",
    "score += discrete_checker(2.5, 0.9036,\n",
    "                          DecisionTree(criterion='gini', max_depth=10),\n",
    "                          \"DecisionTree(criterion='gini', max_depth=10)\",\n",
    "                          x_train, y_train, x_test, y_test\n",
    "                          )\n",
    "\n",
    "score += discrete_checker(2.5, 0.9096,\n",
    "                          DecisionTree(criterion='entropy', max_depth=3),\n",
    "                          \"DecisionTree(criterion='entropy', max_depth=3)\",\n",
    "                          x_train, y_train, x_test, y_test\n",
    "                          )\n",
    "\n",
    "print(\"*** We will check your result for Question 3 manually *** (5 points)\")\n",
    "\n",
    "score += patient_checker(\n",
    "    7.5, 0.91, AdaBoost, {\"n_estimators\": 10},\n",
    "    \"AdaBoost(n_estimators=10)\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "score += patient_checker(\n",
    "    7.5, 0.87, AdaBoost, {\"n_estimators\": 100},\n",
    "    \"AdaBoost(n_estimators=100)\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "score += patient_checker(\n",
    "    5, 0.91, RandomForest,\n",
    "    {\"n_estimators\": 10, \"max_features\": np.sqrt(x_train.shape[1])},\n",
    "    \"RandomForest(n_estimators=10, max_features=sqrt(n_features))\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "score += patient_checker(\n",
    "    5, 0.91, RandomForest,\n",
    "    {\"n_estimators\": 100, \"max_features\": np.sqrt(x_train.shape[1])},\n",
    "    \"RandomForest(n_estimators=100, max_features=sqrt(n_features))\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "score += patient_checker(\n",
    "    5, 0.92, RandomForest,\n",
    "    {\"n_estimators\": 10, \"max_features\": x_train.shape[1]},\n",
    "    \"RandomForest(n_estimators=10, max_features=n_features)\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "print(\"*** We will check your result for Question 6 manually *** (20 points)\")\n",
    "print(\"Approximate score range:\", score, \"~\", score + 25)\n",
    "print(\"*** This score is only for reference ***\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
